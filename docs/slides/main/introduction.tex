\begin{frame}{Introduction}

For this project, a model for a multi-label problem is trained, using the MNIST dataset. Once the model is obtained, adversarial examples can be generated. An adversarial example is an example that is only slightly different from the original and correctly classified example that a model misclassify with a high confidence.

\vspace{0.1in}

The objective of this project is to harness adversarial examples to train a more robust model, by lowering the classification error and the confidence associated with those misclassifications.

\end{frame}

